{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the following questions: What is the impact each individual eigenvector has on the normalized spectral clustering algorithm? While incrementing $k$, the number of clusters, how do the clusterings change? What kinds of datasets can we cluster well using graph spectral clustering? There will be some sections for you to fill in missing code, as well as some short answers based on the output of your code. For all plotting questions, please include an appropriate title and axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import linalg\n",
    "\n",
    "from matplotlib import cm\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate the first set of data we will use for clustering. This data will be drawn from a gaussian mixture.\n",
    "\n",
    "For k clusters, the gaussian mixture is generated by first choosing `n_gaussian` centroids, randomly according to a normal distribution with variance `centroid_var`. For each of these centroids, the `n_pts` points are then chosen according the normal distribution with variance `cluster_var`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1a.** Implement the lines below in the `gaussian_mixture` function to generate data points centered around `n_gaussians` number of centroids, as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture(n_gaussians, n_pts, dim, centroid_var=5, cluster_var=1):\n",
    "    # Set the seed so that the data distribution we work with is 'good'.\n",
    "    np.random.seed(43814828)\n",
    "    \n",
    "    '''\n",
    "    Return a matrix of gaussian vectors, one per row, where each column is a feature.\n",
    "    n_gaussians: integer - number of gaussians to use in the GMM\n",
    "    n_pts: integer - number of points to sample per gaussian\n",
    "    dim: integer - dimensionality of points\n",
    "    centroid_var: float - variance used to pick random means of each Gaussian\n",
    "    cluster_var: float - variance used for each Gaussian of the mixture\n",
    "    '''\n",
    "\n",
    "    # Generate centroids of size (n_gaussians x dim), drawn from a normal\n",
    "    # distribution with mean 0 and variance centroid_var.\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "    centroids = ...\n",
    "    ### END YOUR SOLUTION ###\n",
    "\n",
    "    points_by_gaussian = [\n",
    "    # For each gaussian, sample the points and add the centroid mean.\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "        \n",
    "        \n",
    "        \n",
    "    ### END YOUR SOLUTION ###\n",
    "        for i in range(n_gaussians)\n",
    "    ]\n",
    "\n",
    "    # Join the points as rows.\n",
    "\n",
    "    return np.vstack(points_by_gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carefully read over and understand the cell below. It will generate sliders to visualize the data we just generated above. Learning how to properly plot and visualize our data is important, as it can provide a quick way to check our work. Then, run the cell below to visualize the data we just generated. Play around with the sliders to see how changing the parameters affect the data that is being generated. As a sanity check, the data generated should match the description provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_gaussian(n_gaussians=5, n_pts=4, dim=2, centroid_var=5, cluster_var=1):\n",
    "    n = n_pts * n_gaussians\n",
    "    data = gaussian_mixture(n_gaussians, n_pts, dim, centroid_var)\n",
    "    plt.scatter(*data.T)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.title(\"Sample of Gaussian Mixture\")\n",
    "    plt.show()\n",
    "\n",
    "widget=interactive(interactive_gaussian, {'manual': True},\n",
    "                  n_gaussians=(2, 6, 1),\n",
    "                  n_pts=(2, 10, 1),\n",
    "                  d=fixed(2),\n",
    "                  centroid_var=(1,10,1),\n",
    "                  cluster_var=(1,20,1))\n",
    "\n",
    "widget.children[-2].description='Show Plot'\n",
    "widget.children[-2].style.button_color='lightblue'\n",
    "controls = HBox(widget.children[:-1], layout=Layout(flex_flow='row wrap'))\n",
    "output = widget.children[-1]\n",
    "display(VBox([controls, output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1b.** Describe how each of the values that are being changed by the sliders affect the plot generated. Does this match up with what you expect? If not, revisit your code above to make sure the data is being generated properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** **INSERT ANSWER HERE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1c.** Finally, let's generate the data that we will work with for parts 2 and 3 below. Use the following parameters for the data:\n",
    "\n",
    "<center>n_gaussians=5, n_pts=4, dim=2, centroid_var=10, cluster_var=1</center><br>\n",
    "\n",
    "\n",
    "Then, plot the data. Make sure the data generated matches what you would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "n_gaussians = _\n",
    "n_pts = _\n",
    "n = _  # the total number of data points (used later)\n",
    "dim = _\n",
    "centroid_var = _\n",
    "cluster_var = _\n",
    "\n",
    "data = _\n",
    "\n",
    "plt.scatter(*data.T)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Sample of Gaussian Mixture\")\n",
    "plt.show()\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Graph Spectral Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This section should take the majority of time. Referring back to notes/lecture material will help you understand how the algorithms below work, and what to implement. It will save you time to first have a solid grasp on how the algorithms work, before jumping into implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now get into the heart of the graph spectral clustering algorithm. This section will walk you through the following steps:\n",
    "* Creating a normalized symmetric Laplacian matrix out of data points\n",
    "* Stacking eigenvectors corresponding to the smallest eigenvalues into a matrix\n",
    "* Clustering the rows of this new matrix using k-means\n",
    "* Recovering the clusters for our original points.\n",
    "\n",
    "Below, we have some methods that will help you further understand how the algorithm works. Some of the methods will require you to fill in some code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-means pseudocode is as follows:\n",
    "1. Initialize $k$ points $c_i\\in \\mathbb{R}^d$ with $i=1,\\dots,k$ to be the centroids of each cluster. Initializing with k-means++ picks these centroids to be as distant as possible from each other, to help ensure the correct number of clusters. More details on this in the next section.\n",
    "2. Assign each data point to the closest centroid. The points corresponding to the same centroid form the clusters.\n",
    "3. Move each centroid to the center of its current cluster (using the updated assignments of data).\n",
    "4. Iterate steps 2 and 3 until updating the centroids in step 3 no longer changes the cluster assignments in step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improve upon this k-means algorithm by changing the initialization process. In regular k-means, the centroids are initialized randomly. It has been shown that under random initialization, the worst case runtime can be superpolynomial in runtime (not bounded above by any polynomial, which is bad) and the approximation found can perform arbitrarily poor in comparison to the optimal clustering.\n",
    "\n",
    "So how does the k-means++ initialization of centroids work? The first centroid is chosen arbitrarily at random and every subsequent point is chosen from the remaining points with probability proportional to the square distance from the closest already initialized centroid. It turns out that this initialization guarantees an $O(logk)$ approximation in expectation where k is the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2a.** Fill in the missing code below in the `kmeans` function according to the pseudocode provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, k, iters=100, init=\"++\"):\n",
    "    '''\n",
    "    data: np.ndarray - a [n, d] numpy array of n unique d-dimensional datapoints\n",
    "    k: integer - number of clusters to compute\n",
    "    init: string - one of [\"random\", \"++\"] to either initialize with centroids drawn from a random Gaussian, or to use kmeans++\n",
    "    '''\n",
    "    n, d = data.shape\n",
    "\n",
    "    def new_assns(centroids, as_dist=False):\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "        # List of [norm(data_vector[i], centroid[j]) for j=1..k]\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "        # find index of the nearest centroid for each point\n",
    "        # (argmin returns the *index* of the minimum item, rather than the item itself)\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "\n",
    "    def new_centroids(assns, centroids):\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "        # for j=1..k, find all data columns matching this assignment, then average the cols\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "        return np.vstack(candidate_centroids)\n",
    "\n",
    "    assns = np.zeros(n)\n",
    "\n",
    "    if(init == \"random\"):\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "        # Initialize k x d centroids drawn from a normal distribution with the same variance as the sample variance.\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "    elif(init == \"++\"):\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "        # Initialize k x d centroids through kmeans++\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "    else:\n",
    "        raise ValueError(\"Initialization must be one of ['random', '++']\")\n",
    "\n",
    "\n",
    "    for _ in range(iters):\n",
    "        assns = new_assns(centroids)\n",
    "        centroids = new_centroids(assns, centroids)\n",
    "    return centroids, assns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2b.** In a real world scenario, you likely won't be required to implement k-means by hand, but instead use sklearn's built-in k-means function. To make sure you understand how to use that function, write one line of code below for how you would hypothetically cluster using sklearn's k-means function (feel free to look up documentation online)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next parts, you will implement functions needed for spectral graph clustering. As a reminder, here is the pseudocode for the algorithm:\n",
    "1. Construct a similarity graph according to some well-defined metric (e.g. the Gaussian kernel, Euclidean distance, etc.).\n",
    "2. Compute $L = D - A$, the unnormalized graph Laplacian.\n",
    "3. Compute the symmetric normalized graph Laplacian $L_{sym} = D^{-1/2}LD^{-1/2}$\n",
    "4. Compute the first $k$ eigenvectors of $L_{sym}$, where the first $k$ eigenvectors correspond to the $k$ smallest eigenvalues.\n",
    "5. Construct a matrix $H$ for $L_{sym}$ with the k eigenvectors as the columns, making sure to normalize the rows to have norm 1.\n",
    "6. Run k-means on the rows of $H$ to cluster the vertices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2c.** Fill in the code below. Set the kernel to either the gaussian or exponential kernel, then populate entries in the similarity matrix (remember that diagonals should be zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(data, s=1, metric=\"g\", kernel=None, numOfAtts=None):\n",
    "    '''\n",
    "    Compute an adjacency matrix given a data matrix of size (n, d). Similarities are computed using a kernel specified by the `metric` argument. \n",
    "    data: np.ndarray - (n, d) numpy array containing n datapoints having d dimensions\n",
    "    s: float - scale parameter to be used in each metric. The effect of `s` depends on choice of metric.\n",
    "    metric: one of [\"g\", \"e\", \"k\"] - choose a metric for the data:\n",
    "        \"g\" for Gaussian: d(x, y) = exp(-|x-y|^2 / 2 (s^2)). The scale `s` controls standard deviation. \n",
    "        \"e\" for Exponential: d(x, y) = exp(-|x-y|/s). The scale `s` is the parameter of the exponential.\n",
    "        \"k\" for Kernel: use an arbitrary kernel, given by the `kernel` argument.\n",
    "    kernel: K(x, y, s) -> \\R+ - an arbitrary distance function between two points of the same dimension with a given scale.\n",
    "\n",
    "    @returns an (n, n) np.ndarray adjacency matrix representing a similarity graph for the data\n",
    "    '''\n",
    "    n = len(data)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "    if(metric == \"g\"):\n",
    "        # assign `kernel` to the gaussian kernel\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "    elif(metric == \"e\"):\n",
    "        # assign `kernel` to the exponential kernel\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "    elif(metric == \"k\"):\n",
    "        assert kernel is not None, \"Must pass a kernel function to use kernelized similarity metric\"\n",
    "    else:\n",
    "        raise ValueError(\"Similarity metric must be one of [g, e, k]\")\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "            \n",
    "            \n",
    "            ### END YOUR SOLUTION ###\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Note on Visualizing Matrices**\n",
    "\n",
    "Often times when working with floats, being able to visualize the relative strengths of the entries in the matrix is useful. It's often difficult to see what the relative strengths are when there are a lot of decimals and different powers. A really powerful yet simple way of datavisualization is using color maps. \n",
    "\n",
    "The color map used is viridis. Purple represents the smallest numbers, and it ranges up in backwards rainbow order to yellow, which represents largest numbers used. This scaling changes depending on what range of values the data has. For example, if our data lies in range [-a, a], purple will be -a, 0 would be blue, and a is yellow. On the other hand, if our data was in the range [0,b], purple would be 0 and b would be yellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity Matrix sanity check**\n",
    "\n",
    "Run this cell to visualize what your similarity matrix on the gaussian mixture that you generated earlier looks like. You should expect the similarity matrix to contain zeroes along the diagonal and the matrix to be symmetric. Additionally, since the points for each centroid was generated consecutively, we should also expect strong similarities between consecutive points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = similarity_matrix(data)\n",
    "plt.imshow(similarity)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2d.**  Complete the function to return a Laplacian matrix given the adjacency matrix of a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_matrix(graph_weights):\n",
    "    '''\n",
    "    Return a graph laplacian and degree matrix given the graph's edge weights\n",
    "    graph_weights: np.ndarray - an (n, n) dense graph adjacency matrix\n",
    "    @returns a tuple (laplacian, degree) containing the (n, n) graph laplacian and the (n,) diagonal elements of the degree matrix\n",
    "    '''\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    \n",
    "    \n",
    "    ### END YOUR SOLUTION ###\n",
    "    return laplacian, degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Degree Matrix and Laplacian Matrix sanity check**\n",
    "\n",
    "Run this cell to visualize what your degree matrix and laplacian matrix on the gaussian mixture that you generated earlier looks like. For the degree matrix, you should expect a diagonal matrix. For the laplacian matrix, you should expect positive numbers along the diagonal and negative numbers off the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian, degree = laplacian_matrix(similarity)\n",
    "plt.title(\"Degree matrix\")\n",
    "# print(degree)\n",
    "plt.imshow(np.diag(degree))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Laplacian matrix\")\n",
    "plt.imshow(laplacian)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2e.** Compute the eigenvalues and normalized eigenvectors for the symmetric Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(data, k, with_eigen = False, kmeans_iters = 100, numOfAtts=None, metric = None, init='++', **kwargs):\n",
    "    '''\n",
    "    Args:\n",
    "        data (np.ndarray): (n,d) numpy array consisting of n d-valued points\n",
    "        k (integer): desired number of clusters\n",
    "        with_eigen (:obj:bool, optional) - if True, will also return a tuple (evalues, evecs) of the k Laplacian eigenpairs\n",
    "        metric: one of [\"g\", \"e\", \"k\"] - choose a metric for the data:\n",
    "            \"g\" for Gaussian: d(x, y) = exp(-|x-y|^2 / 2 (s^2)). The scale `s` controls standard deviation.\n",
    "            \"e\" for Exponential: d(x, y) = exp(-|x-y|/s). The scale `s` is the parameter of the exponential.\n",
    "            \"k\" for Kernel: use an arbitrary kernel, given by the `kernel` argument.\n",
    "    Returns:\n",
    "        A list of (n,) integers of the cluster assignments of each data point. If with_eigen=True, also returns eigenvalues and eigenvectors of the Laplacian.\n",
    "    1. use a specialized algorithm to compute indicator vectors\n",
    "    2. cluster the eigenvectors with k-means\n",
    "    ''' \n",
    "\n",
    "    data_sim = similarity_matrix(data, **kwargs, numOfAtts = numOfAtts)\n",
    "    n,d = data_sim.shape\n",
    "    laplacian, degree = laplacian_matrix(data_sim)\n",
    "\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "    # compute the symmetric laplacian\n",
    "                                                  # compute D^(-1/2)\n",
    "                                                  # Construct the symmetric laplacian\n",
    "    # compute the first k eigenvectors/eigenvalues of lsym\n",
    "    # note that because it is symmetric, we can do this efficiently with sp.linalg.eigh\n",
    "                                                  # S, U = sp.linalg.eigh(__, eigvals=__)\n",
    "                                                  # normalize rows of the matrix\n",
    "    ### END YOUR SOLUTION ###\n",
    "\n",
    "    centroids, assns = kmeans(U, k, iters=kmeans_iters, init=init)\n",
    "    if(with_eigen):\n",
    "        return (assns, (S, U))\n",
    "    else:\n",
    "        return assns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2f.** Again, implement graph spectral clustering below, using sklearn. Print the labels of the data generated from the Gaussian mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2g.** Let's compare a kmeans initialization with a kmeans++ visualization, to see if using kmeans++ does actually make a difference. Implement the following code to plot the clustering assignments for GSC with kmeans and GSC with kmeans++, and comment on your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "### BEGIN YOUR SOLUTION ###\n",
    "# Call spectral_clustering with k-means initialization\n",
    "# assns_1, _ =                                               # COMPLETE THIS FUNCTION AND UNCOMMENT THIS LINE\n",
    "print(\"The cluster assignments for 5 clusters with vanilla kmeans: \", assns_1)\n",
    "\n",
    "# Call spectral_clustering with k-means++ initialization\n",
    "# assns_2, _ =                                               # COMPLETE THIS FUNCTION AND UNCOMMENT THIS LINE\n",
    "print(\"The cluster assignments for 5 clusters with kmeans++: \", assns_2)\n",
    "### END YOUR SOLUTION ###\n",
    "\n",
    "cmap = cm.get_cmap(\"tab20\")\n",
    "unif_colors = [cmap(intensity) for intensity in np.linspace(0, 1, n)]\n",
    "data_clusters = [data[assns_1 == clss].T for clss in range(5)]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"K-means initialization\")\n",
    "\n",
    "for j, data_cluster in enumerate(data_clusters):\n",
    "    plt.scatter(*data_cluster, color=unif_colors[j])\n",
    "\n",
    "data_clusters = [data[assns_2 == clss].T for clss in range(5)]\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"K-means++ initialization\")\n",
    "\n",
    "for j, data_cluster in enumerate(data_clusters):\n",
    "    plt.scatter(*data_cluster, color=unif_colors[j])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** **INSERT ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Extracting and Visualizing Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the eigenvectors. The following plot shows eigenvectors (of the Graph Laplacian) in increasing order of the corresponding eigenvalues. The spectral clustering function is called with the exponential kernel (one of the similarity kernels that can be used for spectral clustering), to provide a graph visualizing the eigenvectors.\n",
    "\n",
    "The eigenvector matrix that the function `spectral_clustering` returns has the eigenvectors as the columns of the matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following questions, comment your observations about the extraction and visualization of the eigenvectors that just occurred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (evals, evecs) = spectral_clustering(data, k=n, with_eigen=True, metric=\"e\")\n",
    "plt.imshow(evecs)\n",
    "# print(evecs) ## uncomment this part of the code to see what the entries of the matrix are\n",
    "plt.axis('off')\n",
    "plt.title(\"Eigenvectors of Graph Laplacian\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3a.** Based on this visualization, what do you notice about the eigenvectors of the graph laplacian? The eigenvectors are represented by the columns of the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** **INSERT ANSWER HERE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3b.** Next, plot the eigenvalues of the similarity graph Laplacian. You should generate a plot showing eigenvalue growth rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3b.** What do you notice about the eigenvalues? (Hint: how many eigenvalues are nearly zero?) How can looking at the eigenvalue plot inform us about good $k$'s to choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** **INSERT ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Clusters\n",
    "\n",
    "The cells below will generate subplots, which show classification decisions using a certain number of eigenvectors of the normalized graph Laplacian. These are called cumulative clusters, in the sense that plot $(i+1)$ includes all vectors used to cluster plot $(i)$, plus an additional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cm.get_cmap(\"tab20\")\n",
    "# Note: if n > 20, then multiple indices end up in the same color bin, inducing a seemingly bad clustering\n",
    "unif_colors = [cmap(intensity) for intensity in np.linspace(0, 1, n)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = n_pts\n",
    "c = n_gaussians\n",
    "# r * c = N\n",
    "for i in tqdm(range(1,n+1)):\n",
    "    #evecs = np.fliplr(evecs)\n",
    "    _, assns = kmeans(evecs[:, :i], i, iters=100)\n",
    "    data_clusters = [ data[assns == clss].T for clss in range(i) ]\n",
    "    plt.subplot(r, c, i)\n",
    "    plt.title(f\"{i} evecs\")\n",
    "    plt.gca().set_xticks([], [])\n",
    "    plt.gca().set_yticks([], [])\n",
    "\n",
    "    for j, data_cluster in enumerate(data_clusters):\n",
    "        plt.scatter(*data_cluster, color=unif_colors[j])\n",
    "\n",
    "plt.gcf().set_size_inches(10, 10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3c.** How is the number of clusters $k$ related to how well the algorithm clusters? Which $k$ does the best in this case and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** **INSERT ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to understand how these cluster assignments come to be from the eigenvectors of the graph laplacian. We cluster the rows of this matrix using k means to get the cluster assignments.\n",
    "\n",
    "It's also important to note that we used k-means++ to cluster the rows of our eigenvector matrix instead of the canonical k-means. The difference lies in the convergence times. In k-means++ we initialize the centroids to be as far away as possible and this simple change in initialization helps us reach good solutions. Sometimes if we get really unlucky with our initialization for k-means we end up with either the wrong cluster assignments, or it takes a lot longer to converge to the correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Classifying Harder Datasets\n",
    "\n",
    "For the last part, we will move on to classifying more difficult datasets, in which the clusters are not as obvious. We will observe how well k-means and spectral clustering can identify the moons dataset. From the sklearn documentation, this is a 'simple toy dataset to visualize clustering and classification algorithms. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4a.** Plot a scatterplot of the moons data using the `make_moons` function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moon_data, _ = make_moons(200, noise=0.05)\n",
    "\n",
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4b.** Based on your visualization, why do you think clustering techniques like k-means will not work on the moons dataset? Why is spectral graph clustering suitable to use on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** **INSERT ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to see how K-means will cluster the two moons dataset. Make sure your answer, and the plot below line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, assns = kmeans(moon_data, k=2) # _, assns = \n",
    "\n",
    "moon_data_clusters = [ moon_data[assns == clss].T for clss in range(n_gaussians) ]\n",
    "for j, moon_data_cluster in enumerate(moon_data_clusters):\n",
    "    plt.scatter(*moon_data_cluster, color=unif_colors[j], label=f\"{j}\")\n",
    "\n",
    "plt.title(\"Clustering with K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4c.** Fill in the code below to implement a slider for values of $\\sigma$, which range from 0.01 to 1, with step size of 0.01. Play around with the slider. You should be able to find a range of values which will cluster the two moons dataset correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_gaussian_kernel(data=moon_data, sigma=0.01):\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    ### END YOUR SOLUTION ###\n",
    "    plt.title(\"Spectral Clustering with Gaussian Kernel, $\\sigma=$\" + str(sigma))\n",
    "\n",
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4d.** Comment your observations on how the hyperparameter $\\sigma$ affects the cluster assignments. What values of $\\sigma$ cluster the two moons correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** **INSERT ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4e.** How do the values of $\\sigma$ affect the cluster assignments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** **INSERT ANSWER HERE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Conclusion and Takeaways\n",
    "\n",
    "Hopefully, this project has provided you with some more intuition for the graph spectral clustering algorithm's structure and some experience in tuning its parameters for particular applications. To wrap up, we present a few more thoughts on using this algorithm in a practical setting.\n",
    "\n",
    "One important caveat when using GSC in practice is that, unlike other clustering algorithms such as k-means, it cannot easily be used to predict a new data point's cluster. This makes it ideal for applications where all data is present from the beginning. This includes tasks like image segmentation, which is the context in which the algorithm was originally developed. Due to the algorithm's relation to ratio cut, it is also best suited for situations where clusters are expected or desired to be of roughly equal size.\n",
    "\n",
    "With that said, graph spectral clustering has many applications in ML, including exploratory data analysis, computer vision, and speech processing. Additionally, it can be applied to a variety of fields, such as statistics, computer science, biology, psychology, etc. One really cool example of graph spectral clustering applied to genetic ancestry can be found [here](http://www.stat.cmu.edu/~roeder/publications/leeaoas.pdf).\n",
    "\n",
    "All in all, graph spectral clustering is a powerful tool that we hope has been added to your arsenal for future use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
