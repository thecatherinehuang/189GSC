{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the following questions: What is the impact each individual eigenvector has on the normalized spectral clustering algorithm? While incrementing $k$, the number of clusters, how do the clusterings change? What kinds of datasets can we cluster well using spectral clustering? There will be some sections for you to fill in missing code, as well as some short answers based on the output of your code. (For all plotting questions, please include a title and axis labels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import linalg\n",
    "from matplotlib import cm\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans, SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate the first set of data we will use for clustering. This data will be drawn from a gaussian mixture.\n",
    "\n",
    "For k clusters, the gaussian mixture is generated by first choosing `n_gaussian` centroids, randomly according to a normal distribution with variance `centroid_var`. For each of these centroids, the `n_pts` points are then chosen according the normal distribution with variance `cluster_var`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1a.** Implement the lines below in the `gaussian_mixture` function to generate data points centered around `n_gaussians` number of centroids, as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture(n_gaussians, n_pts, dim, centroid_var=5, cluster_var=1):\n",
    "    '''\n",
    "    Return a matrix of gaussian vectors, one per row, where each column is a feature.\n",
    "    n_gaussians: integer - number of gaussians to use in the GMM\n",
    "    n_pts: integer - number of points to sample per gaussian\n",
    "    dim: integer - dimensionality of points\n",
    "    centroid_var: float - variance used to pick random means of each Gaussian\n",
    "    cluster_var: float - variance used for each Gaussian of the mixture\n",
    "    '''\n",
    "\n",
    "    # Generate centroids of size (n_gaussians x dim), drawn from a normal\n",
    "    # distribution with mean 0 and variance centroid_var.\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    \n",
    "    \n",
    "    ### END YOUR SOLUTION ###\n",
    "\n",
    "    points_by_gaussian = [\n",
    "    # For each gaussian, sample the points and add the centroid mean.\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "        \n",
    "        \n",
    "    ### END YOUR SOLUTION ###\n",
    "        for i in range(n_gaussians)\n",
    "    ]\n",
    "\n",
    "    # Join the points as rows.\n",
    "\n",
    "    return np.vstack(points_by_gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carefully read over and understand the cell below. It will generate sliders to visualize the data we just generated above. Learning how to properly plot and visualize our data is important, as it can provide a quick way to check our work. Then, run the cell below to visualize the data we just generated. Play around with the sliders to see how changing the parameters affect the data that is being generated. As a sanity check, the data generated should match the description provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_gaussian(n_gaussians=5, n_pts=4, d=2, centroid_var=5, cluster_var=1):\n",
    "    n = n_pts * n_gaussians\n",
    "    data = gaussian_mixture(n_gaussians, n_pts, d, centroid_var)\n",
    "    plt.scatter(*data.T)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.title(\"Sample of Gaussian Mixture\")\n",
    "    plt.show()\n",
    "\n",
    "widget=interactive(interactive_gaussian, {'manual': True},\n",
    "                  n_gaussians=(2, 6, 1),\n",
    "                  n_pts=(2, 10, 1),\n",
    "                  d=fixed(2),\n",
    "                  centroid_var=(1,10,1),\n",
    "                  cluster_var=(1,20,1))\n",
    "\n",
    "widget.children[-2].description='Show Plot'\n",
    "widget.children[-2].style.button_color='lightblue'\n",
    "controls = HBox(widget.children[:-1], layout=Layout(flex_flow='row wrap'))\n",
    "output = widget.children[-1]\n",
    "display(VBox([controls, output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1b.** Describe how each of the values that are being changed by the sliders affect the plot generated. Does this match up with what you expect? If not, revisit your code above to make sure the data is being generated properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "_Insert answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1c.** Finally, let's generate the data that we will work with for parts 2 and 3 below. Use the following parameters for the data:\n",
    "\n",
    "<center>n_gaussians=5, n_pts=4, dim=2, centroid_var=10, cluster_var=1</center><br>\n",
    "\n",
    "Then, plot the data. Make sure the data generated matches what you would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN YOUR SOLUTION ###\n",
    "n_gaussians = _\n",
    "n_pts = _\n",
    "n = _  # the total number of data points (used later)\n",
    "d = _\n",
    "centroid_var = _\n",
    "cluster_var = _\n",
    "\n",
    "data = _\n",
    "\n",
    "plt.scatter(*data.T)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Sample of Gaussian Mixture\")\n",
    "plt.show()\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Spectral Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now get into the heart of the spectral clustering algorithm. Below, we have some methods that will help you further understand how the algorithm works. Some of the methods will require some code to be filled in.\n",
    "\n",
    "This section will walk you through the steps of take your data points to create a normalized symmetric Laplacian matrix, stack the eigenvectors corresponding to the smallest eigenvalues into a matrix, then cluster the rows of this new matrix using k-means. From the cluster assignments found by k-means, we can then recover the clusters for our original points.\n",
    "\n",
    "**Note:** This section should take the majority of time. Referring back to notes/lecture material will help you understand how the algorithms below work, and what to implement. It will save you time to first understand the algorithms before jumping into implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-means algorithm is as follows:\n",
    "1. Initialize $k$ points $c_i\\in \\mathbb{R}^d$ with $i=1,\\dots,k$ to be the centroids of each cluster. Initializing with k-means++ picks these centroids to be as distant as possible from each other, to help ensure the correct number of clusters. More details on this in the next section.\n",
    "2. Assign each data point to the closest centroid. The points corresponding to the same centroid form the clusters.\n",
    "3. Move each centroid to the center of its current cluster (using the updated assignments of data).\n",
    "4. Iterate steps 2 and 3 until updating the centroids in step 3 no longer changes the cluster assignments in step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improve upon this k-means algorithm by changing the initialization process. In regular k-means, the centroids are initialized randomly. It has been shown that under random initialization, the worst case runtime can be superpolynomial in runtime (not bounded above by any polynomial, bad) and the approximation found can be arbitrarily bad in comparison to the optimal clustering.\n",
    "\n",
    "So how does the k-means++ initialization of centroids work? The first centroid is chosen arbitrarily at random and every subsequent point is chosen from the remaining points with probability proportional to the square distance from the closest already initialized centroid. It turns out that this initialization guarantees a O(log k) approximation in expectation where k is the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2a.** Fill in the missing code below in the `kmeans` function according to the pseudocode provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, k, iters=100, init=\"++\"):\n",
    "    '''\n",
    "    data: np.ndarray - a [n, d] numpy array of n unique d-dimensional datapoints\n",
    "    k: integer - number of clusters to compute\n",
    "    init: string - one of [\"random\", \"++\"] to either initialize with centroids drawn from a random Gaussian, or to use kmeans++\n",
    "    '''\n",
    "    n, d = data.shape\n",
    "\n",
    "    def new_assns(centroids, as_dist=False):\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "        # List of [norm(data_vector[i], centroid[j]) for j=1..k]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "        # find index of the nearest centroid for each point\n",
    "        # (argmin returns the *index* of the minimum item, rather than the item itself)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "\n",
    "    def new_centroids(assns, centroids):\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "        # for j=1..k, find all data columns matching this assignment, then average the cols\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "        return np.vstack(candidate_centroids)\n",
    "\n",
    "    assns = np.zeros(n)\n",
    "\n",
    "    if(init == \"random\"):\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "        # Initialize k x d centroids drawn from a normal distribution with the same variance as the sample variance.\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "    elif(init == \"++\"):\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "        # Initialize k x d centroids through kmeans++\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "    else:\n",
    "        raise ValueError(\"Initialization must be one of ['random', '++']\")\n",
    "\n",
    "\n",
    "    for _ in range(iters):\n",
    "        assns = new_assns(centroids)\n",
    "        centroids = new_centroids(assns, centroids)\n",
    "    return centroids, assns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2b.** In a real world scenario, you likely won't be required to implement k-means by hand, but instead use sklearn's built-in k-means function. To make sure you understand how to use that function, write one line of code below for how you would hypothetically cluster using sklearn's k-means function (feel free to look up documentation online)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next parts, you will implement functions needed for spectral graph clustering. As a reminder, here is the pseudocode for the algorithm:\n",
    "1. Construct a similarity graph according to some well-defined metric (e.g. the Gaussian kernel, Euclidean distance, etc.).\n",
    "2. Compute $L = D - A$, the unnormalized graph Laplacian.\n",
    "3. Compute the symmetric normalized graph Laplacian $L_{sym} = D^{-1/2}LD^{-1/2}$\n",
    "4. Compute the first $k$ eigenvectors of $L_{sym}$, where the first $k$ eigenvectors correspond to the $k$ smallest eigenvalues.\n",
    "5. Construct a matrix $H$ for $L_{sym}$ with the k eigenvectors as the columns, making sure to normalize the rows to have norm 1.\n",
    "6. Run k-means on the rows of $H$ to cluster the vertices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2c.** Fill in the code below. Set the kernel to either the gaussian or exponential kernel, then populate entries in the similarity matrix (remember that diagonals should be zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(data, s=1, metric=\"g\", kernel=None, numOfAtts=None):\n",
    "    '''\n",
    "    Compute an adjacency matrix given a data matrix of size (n, d). Similarities are computed using a kernel specified by the `metric` argument. \n",
    "    data: np.ndarray - (n, d) numpy array containing n datapoints having d dimensions\n",
    "    s: float - scale parameter to be used in each metric. The effect of `s` depends on choice of metric.\n",
    "    metric: one of [\"g\", \"e\", \"k\"] - choose a metric for the data:\n",
    "        \"g\" for Gaussian: d(x, y) = exp(-|x-y|^2 / 2 (s^2)). The scale `s` controls standard deviation. \n",
    "        \"e\" for Exponential: d(x, y) = exp(-|x-y|/s). The scale `s` is the parameter of the exponential.\n",
    "        \"eps\" for Epsilon neighbors: d(x, y) = |x-y| if less than epsilon, 0 otherwise\n",
    "        \"k\" for Kernel: use an arbitrary kernel, given by the `kernel` argument.\n",
    "    kernel: K(x, y, s) -> \\R+ - an arbitrary distance function between two points of the same dimension with a given scale.\n",
    "\n",
    "    @returns an (n, n) np.ndarray adjacency matrix representing a similarity graph for the data\n",
    "    '''\n",
    "    n = len(data)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "    if(metric == \"g\"):\n",
    "        # assign `kernel` to the gaussian kernel\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "    elif(metric == \"e\"):\n",
    "        # assign `kernel` to the exponential kernel\n",
    "        ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "        \n",
    "        \n",
    "        ### END YOUR SOLUTION ###\n",
    "    elif(metric == \"eps\"):\n",
    "        def eps_threshold(x, eps):\n",
    "            return x if x < eps else 0\n",
    "        kernel = lambda x, y, s : eps_threshold(np.linalg.norm(x-y), s)\n",
    "    elif(metric == \"k\"):\n",
    "        assert kernel is not None, \"Must pass a kernel function to use kernelized similarity metric\"\n",
    "    else:\n",
    "        raise ValueError(\"Similarity metric must be one of [g, e, k]\")\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            ### END YOUR SOLUTION ###\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2d.**  Complete the function to return a Laplacian matrix given the adjacency matrix of a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_matrix(graph_weights):\n",
    "    '''\n",
    "    Return a graph laplacian and degree matrix given the graph's edge weights\n",
    "    graph_weights: np.ndarray - an (n, n) dense graph adjacency matrix\n",
    "    @returns a tuple (laplacian, degree) containing the (n, n) graph laplacian and the (n,) diagonal elements of the degree matrix\n",
    "    '''\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    \n",
    "    \n",
    "    ### END YOUR SOLUTION ###\n",
    "    return laplacian, degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2e.** Compute the eigenvalues and normalized eigenvectors for the symmetric Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(data, k, with_eigen = False, kmeans_iters = 100, numOfAtts=None, metric = None, **kwargs):\n",
    "    '''\n",
    "    Args:\n",
    "        data (np.ndarray): (n,d) numpy array consisting of n d-valued points\n",
    "        k (integer): desired number of clusters\n",
    "        with_eigen (:obj:bool, optional) - if True, will also return a tuple (evalues, evecs) of the k Laplacian eigenpairs\n",
    "        metric: one of [\"g\", \"e\", \"k\"] - choose a metric for the data:\n",
    "            \"g\" for Gaussian: d(x, y) = exp(-|x-y|^2 / 2 (s^2)). The scale `s` controls standard deviation.\n",
    "            \"e\" for Exponential: d(x, y) = exp(-|x-y|/s). The scale `s` is the parameter of the exponential.\n",
    "            \"k\" for Kernel: use an arbitrary kernel, given by the `kernel` argument.\n",
    "    Returns:\n",
    "        A list of (n,) integers of the cluster assignments of each data point. If with_eigen=True, also returns eigenvalues and eigenvectors of the Laplacian.\n",
    "    1. use a specialized algorithm to compute indicator vectors\n",
    "    2. cluster the eigenvectors with k-means\n",
    "    ''' \n",
    "\n",
    "    data_sim = similarity_matrix(data, **kwargs, numOfAtts = numOfAtts)\n",
    "    n,d = data_sim.shape\n",
    "    laplacian, degree = laplacian_matrix(data_sim)\n",
    "\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "    # compute the symmetric laplacian\n",
    "                                                # compute D^(-1/2)\n",
    "                                                # Construct the symmetric laplacian\n",
    "    # compute the first k eigenvectors/eigenvalues of lsym\n",
    "    # note that because it is symmetric, we can do this efficiently with sp.linalg.eigh\n",
    "                                                # S, U = sp.linalg.eigh(__, eigvals=__)\n",
    "                                                # normalize rows of the matrix\n",
    "    ### END YOUR SOLUTION ###\n",
    "\n",
    "    centroids, assns = kmeans(U, k, iters=kmeans_iters)\n",
    "    if(with_eigen):\n",
    "        return (assns, (S, U))\n",
    "    else:\n",
    "        return assns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2f.** Again, implement spectral clustering below, using sklearn. Print the labels of the data generated from the Gaussian mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Extracting and Visualizing Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the eigenvectors. The following plot shows eigenvectors (of the Graph Laplacian) in increasing order of the corresponding eigenvalues. The spectral clustering function is called with the exponential kernel (one of the similarity kernels that can be used for spectral clustering), to provide a graph visualizing the eigenvectors.\n",
    "\n",
    "The color map used is viridis. Purple represents large negative numbers, and it ranges up in backwards rainbow order to yellow, which represents large positive numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following questions, comment your observations about the extraction and visualization of the eigenvectors that just occurred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assns, _ = spectral_clustering(data, k=5, with_eigen=True, metric=\"e\")\n",
    "print(\"The cluster assignments for 5 clusters: \", assns)\n",
    "_, (evals, evecs) = spectral_clustering(data, k=n, with_eigen=True, metric=\"e\")\n",
    "plt.imshow(evecs)\n",
    "# print(evecs) ## uncomment this part of the code to see what the entries of the matrix are\n",
    "plt.axis('off')\n",
    "plt.title(\"Eigenvectors of Graph Laplacian\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3a.** Based on this visualization, what do you notice about the eigenvectors of the graph laplacian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "_Insert answer here_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3b.** Next, plot the eigenvalues of the similarity graph Laplacian. You should generate a plot showing eigenvalue growth rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3b.** What do you notice about the eigenvalues? (Hint: how many eigenvalues are nearly zero?) How can looking at the eigenvalue plot inform us about good $k$'s to choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "_Insert answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Clusters\n",
    "\n",
    "The cells below will generate subplots, which show classification decisions using a certain number of eigenvectors of the normalized graph Laplacian. These are called cumulative clusters, in the sense that plot $(i+1)$ includes all vectors used to cluster plot $(i)$, plus an additional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cm.get_cmap(\"tab20\")\n",
    "# Note: if n > 20, then multiple indices end up in the same color bin, inducing a seemingly bad clustering\n",
    "unif_colors = [cmap(intensity) for intensity in np.linspace(0, 1, n)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = n_pts\n",
    "c = n_gaussians\n",
    "# r * c = N\n",
    "for i in tqdm(range(1,n+1)):\n",
    "    #evecs = np.fliplr(evecs)\n",
    "    _, assns = kmeans(evecs[:, :i], i, iters=100)\n",
    "    data_clusters = [ data[assns == clss].T for clss in range(i) ]\n",
    "    plt.subplot(r, c, i)\n",
    "    plt.title(f\"{i} evecs\")\n",
    "    plt.gca().set_xticks([], [])\n",
    "    plt.gca().set_yticks([], [])\n",
    "\n",
    "    for j, data_cluster in enumerate(data_clusters):\n",
    "        plt.scatter(*data_cluster, color=unif_colors[j])\n",
    "\n",
    "plt.gcf().set_size_inches(10, 10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3c.** How is the number of clusters $k$ related to how well the algorithm clusters? Which $k$ does the best in this case and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "_Insert answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to understand how these cluster assignments come to be from the eigenvectors of the graph laplacian. We cluster the rows of this matrix using k means to get the cluster assignments.\n",
    "\n",
    "It's also important to note that we used k-means++ to cluster the rows of our eigenvector matrix instead of the canonical k-means. The difference lies in the convergence times. In k-means++ we initialize the centroids to be as far away as possible and this simple change in initialization helps us reach good solutions. Sometimes if we get really unlucky with our initialization for k-means we end up with either the wrong cluster assignments, or it takes a lot longer to converge to the correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Classifying Harder Datasets\n",
    "\n",
    "For the last part, we will move on to classifying more difficult datasets, in which the clusters are not as obvious. We will observe how well k-means and spectral clustering can identify the moons dataset. From the sklearn documentation, this is a 'simple toy dataset to visualize clustering and classification algorithms. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4a.** Plot a scatterplot of the moons data using the `make_moons` function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moon_data, _ = make_moons(200, noise=0.05)\n",
    "\n",
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4b.** Based on your visualization, why do you think clustering techniques like k-means will not work on the moons dataset? Why is spectral graph clustering suitable to use on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "_Insert answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to see how K-means will cluster the two moons dataset. Make sure your answer, and the plot below line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, assns = kmeans(moon_data, k=2) # _, assns = \n",
    "\n",
    "moon_data_clusters = [ moon_data[assns == clss].T for clss in range(n_gaussians) ]\n",
    "for j, moon_data_cluster in enumerate(moon_data_clusters):\n",
    "    plt.scatter(*moon_data_cluster, color=unif_colors[j], label=f\"{j}\")\n",
    "\n",
    "plt.title(\"Clustering with K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4c.** Fill in the code below to implement a slider for values of $\\sigma$, which range from 0.01 to 1, with step size of 0.01. Play around with the slider. You should be able to find a range of values which will cluster the two moons dataset correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_gaussian_kernel(data=moon_data, sigma=0.01):\n",
    "    ### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR SOLUTION ###\n",
    "    plt.title(\"Spectral Clustering with Gaussian Kernel, $\\sigma=$\" + str(sigma))\n",
    "\n",
    "### BEGIN YOUR SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4d.** Comment your observations on how the hyperparameter $\\sigma$ affects the cluster assignments. What values of $\\sigma$ cluster the two moons correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "_Insert answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4e.** How do the values of $\\sigma$ affect the cluster assignments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "_Insert answer here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
